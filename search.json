[
  {
    "objectID": "posts/tennis/FourthSet_NiceEasyMaps.html",
    "href": "posts/tennis/FourthSet_NiceEasyMaps.html",
    "title": "Fourth Set: Nice, Easy Mapping",
    "section": "",
    "text": "Mapping made easy!\nIn a previous post I outlined how we can take a set of addresses scraped from Tennis NZs website and geocode them to get latitude and longitude coordinates. I’ve tried out a bunch of different techniques to analyse this dataset, including drive isodistances and point density mapping\nIn this post, I’ll be testing out a really easy way to make a map based on a set of coordinates, inspired by this LinkedIn post by Isaac Bain. In this post, Isaac shows how we can use the rcityviews package to take a set of coordinates and feed them into the rcityviews::cityview() function to return a nice map.\nLets take the first 6 tennis club addresses of set we scraped earlier.\n\n\nCode\nlibrary(here)\nlibrary(dplyr)\nlibrary(reactable)\ndf &lt;- read.csv(here(\"inputs\",\"tennis\",\"tennis_coords.csv\"))\n\n#We know the geocoding failed on some addresses, so we'll exclude those\ndf_clean &lt;- df |&gt;\n  filter(!is.na(latitude) | !is.na(longitude)) |&gt;\n  head()\n\ndf_clean |&gt; \n  reactable(\n    bordered = TRUE,\n    highlight = TRUE\n  ) \n\n\n\n\n\n\nThen, it’s super straight forward to make a map! Here’s the code for a single address. Simply use the new_city function to set the centre location of our map to our desired coordinates, and then use the cityview function to render the image. the rcityviews package does all of the hard work of connecting to OpenStreetMap and pulling the map data.\n\n\nCode\nlibrary(rcityviews)\n\n # Create a new city object\ncity &lt;- new_city(\n  name = df$name[1],\n  country = \"New Zealand\",\n  lat = df$latitude[1],\n  long = df$longitude[1]\n)\n\np &lt;- cityview(\n  name = city,\n  zoom = 4, #we'll set the zoom a bit closer so we can see the courts. \n  license = FALSE,\n  theme = 'modern',\n  filename=here(\"example_nice_map.png\")\n)\n\nprint(p)\n\n\n\n\n\nExample nice map. Hopefully we can see the tennis courts dead centre.\n\n\nAnd here’s the code for running through our set of addresses. We can set our own style for the maps manually.\n\n\nCode\n# set theme for map appearance\nmyTheme &lt;- list(\n  colors = list(\n   \"background\" = \"#e6ddd6\",\n      \"water\" = \"#656c7c\",\n      \"landuse\" = \"#7c9c6b\",\n      \"contours\" = \"#e6ddd6\",\n      \"streets\" = \"#fafafa\",\n      \"rails\" = c(\"#fafafa\", \"#e6ddd6\"),\n      \"buildings\" = \"#eb3e20\",\n      \"text\" = \"#000000\",\n      \"waterlines\" = \"#656c7c\"\n  ),\n  font = list(\n    \"family\" = \"Imbue\",\n      \"face\" = \"plain\",\n      \"scale\" = 3\n  ),\n  size = list(\n    borders = list(\n      contours = 0.15,\n      water = 0.4,\n      canal = 0.5,\n      river = 0.6\n    ),\n    streets = list(\n      path = 0.2,\n      residential = 0.3,\n      structure = 0.35,\n      tertiary = 0.4,\n      secondary = 0.5,\n      primary = 0.6,\n      motorway = 0.8,\n      rails = 0.65,\n      runway = 3\n    )\n  )\n)\n\n# Loop through each tennis club address\nfor (i in 1:nrow(df_clean)) {\n  # Extract tennis club details\n  name &lt;- df_clean$name[i]\n  lat &lt;- df_clean$latitude[i]\n  long &lt;- df_clean$longitude[i]\n\n  # Create a new city object\n  city &lt;- new_city(\n    name = name,\n    country = \"New Zealand\",\n    lat = lat,\n    long = long\n    )\n\n# Produce the cityview\n  p &lt;- cityview(\n    name = city,\n    zoom = 4,\n    license = FALSE,\n    theme = myTheme\n    )\n  \n  output_filename &lt;- here('inputs','tennis','maps',paste0(name,\".png\"))\n\n  ggsave(\n    filename = output_filename,\n    plot = p,\n    height = 500,\n    width = 500,\n    units = \"mm\",\n    dpi = 300\n    )\n}\n\n\nAnd here’s the result!\n\n\n\n\n\n\n\n\n\nA very easy to use library that has awesome options for styling the maps!"
  },
  {
    "objectID": "posts/tennis/FirstSet_Webscraping_and_GeoCoding.html",
    "href": "posts/tennis/FirstSet_Webscraping_and_GeoCoding.html",
    "title": "First Set: Web Scraping and Geocoding",
    "section": "",
    "text": "Scraping the Barrel\nThere’s been so many times that I’ve wanted to pull location information from a website or file containing a set of addresses. This can be achieved by many hours of manual copy and pasting, a task that may even be impossible depending on the scale of the data. We can save time and sanity by making our computers do the work for us. Websites are (usually) highly organised and structured, making them perfect for programming tasks. In this post, I’ll show you how you can do this.\nFirst, we want to grab data from the website we are interested in. Here’s a screen shot of Tennis NZs website showing where to play. You can see it contains a map with tennis clubs highlighted, but the information isn’t immediately accessible. You could scroll through each entry on the left (&gt;300 clubs) and copy and paste the information out, but that’s not fun.\n\n\n\nMap of Tennis Clubs in Auckland\n\n\nThere are two ways we can pull this information. We can use the webscraper package rvest to directly pull the html from the site using read_html(website_url). For our use case ( and to ensure reproducibility), I’ll download a copy of the html directly - simply right click on the page, and click ‘save as’. We’ll have the the file Tennis NZ - Where to Play.html available for us to look at in more detail.\nThe code below loads this .html file. It then uses tools from the rvest package to grab the specific bits of information we are interested in. By right clicking on a webpage and clicking inspect, you can focus on the parts of the website that you are interested in. In our case, we will be pulling the contents of store-point-results \n\n\nCode\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(stringr)\n# Read the HTML content from the file\nhtml_content &lt;- read_html(here(\"inputs\",\"tennis\",\"Tennis NZ - Where to Play.html\"))\n\n# Extract addresses from elements with the class 'storepoint-address' from the HTML content\nstorepoint_address_contents &lt;- html_content |&gt;\n  # Select elements with the class 'storepoint-address'\n  html_elements('.storepoint-address') |&gt;\n  # Extract text content from the selected elements and trim any surrounding whitespace\n  html_text(trim = TRUE)\n\n# Also get the tags of elements with the class 'storepoint-name' from the HTML content\ntag_text_contents &lt;- html_content |&gt;\n  # Select elements with the class 'storepoint-name'\n  html_elements('.storepoint-name') |&gt;\n  # Extract text content from the selected elements and trim any surrounding whitespace\n  html_text(trim = TRUE)\n\n# Combine the extracted name and address data into a data frame\ntennis_df &lt;- data.frame(\n  # Assign the extracted names to the 'name' column\n  name = tag_text_contents,\n  # Assign the extracted addresses to the 'storepoint_address' column\n  storepoint_address = storepoint_address_contents,\n  # Prevent strings from being converted to factors\n  stringsAsFactors = FALSE\n) \n\n\n\n\n\n\n\n\nAs happens in 90% of cases, there are some data quality issues. For example, there’s double commas, with no content in between them. Before we can do anything with this data, we will need to fix these issues (geocoding addresses that are broken is futile!).\nMy code below fixes some of the issues I found in the scraped data. As an aside, regex is a game changer for cleaning data. As a Data Scientist, it’s very rare that I’ll have a day where I haven’t used some form of regex.\n\n\nCode\ntennis_df_clean &lt;- tennis_df |&gt;\n   # Add space between a lowercase letter followed by an uppercase letter\n  mutate(storepoint_address = str_replace_all(storepoint_address, \"([a-z])([A-Z])\",paste0(\"\\\\1\",\" \",\"\\\\2\" ))) |&gt;\n  # Add space between a lowercase letter followed by a number\n  mutate(storepoint_address = str_replace_all(storepoint_address, \"([a-z])([0-9])\",paste0(\"\\\\1\",\" \",\"\\\\2\" ))) |&gt;\n  # Replace the abbreviation \"Cnr\" with \"Corner\"\n  mutate(storepoint_address = str_replace_all(storepoint_address, \"Cnr\",\"Corner\")) |&gt;\n  # Remove any spaces that appear before commas\n  mutate(storepoint_address = str_replace_all(storepoint_address, \" ,\",\"\")) |&gt;\n  # Replace any double commas with a single comma\n  mutate(storepoint_address = str_replace_all(storepoint_address, \",,\",\",\")) |&gt;\n  # Replace the repeated \"Remuera Remuera Remuera\" with a single \"Remuera\"\n  mutate(storepoint_address = str_replace_all(storepoint_address, \"Remuera Remuera Remuera\",\"Remuera\")) \n\n\nNow we have our clean scraped data, we can focus on geocoding!\n\n\nGeocode with one Simple Step\nWebscraping is a simple skill opens up a world of possibilies for geospatial analysis. With the set of addresses at hand, we can add on an additional step to augment our newly pulled data with additional information. In our case, unfortutely we were unable to find any data pertaining to longitude and latidude of tennis clubs. Luckily, the tidygeocoder package makes it incredibly straight forward to get this information.\nHere’s a simple chunk of code that will go through a dataset of addresses and grab the longitude and latitude.\n\n\nCode\nlibrary(tidygeocoder)\n\n#This is where we will store our results\ncoord_main &lt;- data.frame()\n\n#Iterate through each row of the tennis data\nfor(i in 1:nrow(tennis_df_clean)){\n  #We'll set our system to sleep before each run.\n  #This is to be kind to Nominatim and not bombard them with requests\n  Sys.sleep(1)\n\n  #Grab the specific row of the data\n  df_i&lt;-tennis_df_clean[i,]\n  \n  #© openrouteservice.org by HeiGIT | Map data © OpenStreetMap contributors\n  # Use the geocode function to get latitude and longitude for the address in the 'storepoint_address' column\n  coords_df &lt;- df_i |&gt; \n    # Geocode the addresses using the Nominatim (osm) method\n    geocode(address = storepoint_address, method = 'osm') |&gt;\n    # Rename the columns 'lat' and 'long' to 'latitude' and 'longitude' respectively\n    rename(latitude = lat, longitude = long)\n\n  # Save the combined data frame to a CSV file. \n  # Using append means that it will write each line 1 at a time, saving our work as we go.\n  # This is useful if you have a big batch of addresses as it means you're less likely to lose work if the process breaks halfway through (API calls can be costly). \n  write.table(\n    coords_df, # Data frame to write\n    here(\"inputs\",\"tennis\",\"tennis_coords.csv\"), # File to write to.  \n    sep = \",\", # Use a comma as the separator\n    col.names = !file.exists(here(\"inputs\",\"tennis\",\"tennis_coords.csv\")), # Include column names only if the file doesn't already exist\n    append = TRUE, # Append the data to the file if it already exists\n    row.names = FALSE # Do not write row names\n  )\n\n  # Combine the new data with the main coordinates data frame\n  coord_main &lt;- bind_rows(coord_main, coords_df)\n\n  # Print a message to the console indicating the completion of processing for each address\n  cat(\"\\n\", coords_df$name, \" complete. (\", coords_df$latitude, \", \", df_save$longitude, \")\")\n}\n\n\nAnd here’s the results!\n\n\n\n\n\n\nNote, some addresses may not get a result, it depends on the quality of the addresses and the geocoding service that you use. In our case, we know the data is patchy and I’ve used open street map to geocode. Other services can be found here.\n\n\nThe Fruits of Our (minimal) Labour\nWe’ve managed to pull publically available address data, use a geocoder to get the longitude and latitude coordinates, now it time to see what we’ve got. Through a combination of ggplot2 and ggiraph, we are able to quickly visualise the newly process data points on an interactive map of Auckland. See the code below.\n\n\nCode\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(ggiraph)\n#load our tennis club coordinates data\ncoords &lt;- read.csv(here(\"inputs\",\"tennis\",\"tennis_coords.csv\")) |&gt;\n  filter(!is.na(latitude)|!is.na(longitude)) |&gt;\n  mutate(name = str_replace_all(name,\"'\",\"\"))\n#here's a shapefile for NZ that I prepared earlier\nnz_map &lt;- read_sf(here(\"inputs\",\"shapefiles\",\"NZ_res01.shp\"))\n\ng &lt;- ggplot() +\n  # Add our map of NZ\n  geom_sf(\n    data = nz_map,\n    colour = \"black\",\n    linewidth = 0.2\n  ) +\n    # Add the tennis club locations.\n  #ggiraph makes things interactive.\n  geom_point_interactive(\n    data = coords,\n    aes(\n      x = longitude,\n      y = latitude,\n      tooltip = name,\n      data_id = name\n      ),\n    hover_nearest = T,\n    shape=21,\n    size=10,\n    colour = \"white\",\n    fill = \"#049030\",\n    alpha=0.7\n    ) +\n    # Clip the map just to the main Auckland Area\n  coord_sf(\n    xlim = c(174.5, 175.2), \n    ylim = c(-37.1, -36.6) \n  ) +\n    #Style\n  theme_void() +\n  theme(title = element_text(size=20)) +\n  #Title\n  ggtitle(\"Tennis Courts in Auckland, New Zealand\")\n\n# wrap it in the girafe function which makes it interactive\ngirafe(\n  ggobj = g,\n  # set options for styling\n    options = list(\n    opts_hover(css = \"fill:yellow;stroke:black;stroke-width:3px;\")\n    )\n  )\n\n\n\n\n\n\nThat’s a wrap. Now that we have our data, there’s a huge number of analysis opportunities. We could look at underlying population demographics to understand access, combine it with drive times data to understand potential catchment areas,build up our visualisation to be fully customisable and interactive. At the very least, hopefully this post will save someone some time away from the computer, manually copying and pasting addresses from a website into google maps!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! I’m Steven Turnbull, a data scientist with a wide range of interests. I use this site to post about random things that I find interesting!\n\nSome Things I Have Worked On\n\nPhD Thesis in Education investigating student pathways through STEM\nCOVID-19 Network Modelling\nNetwork and Geospatial Analysis\nData Visualisation\nData Infrastructure (ETL/ELT Pipelines, Dashboarding)\n\n\n\nExample Research Publications\n\n\nEmily P.Harvey, Joel A.Trent, Frank Mackenzie, Steven M. Turnbull and Dion R.J.O’Neale (2022). Calculating incidence of Influenza-like and COVID-like symptoms from Flutracking participatory survey data. MethodsX. Link\nTurnbull, S. M., Hobbs, M, Grey, L., Harvey, E., Scarrold, W., & O’Neale, D. (2022). Investigating the transmission risk of infectious disease outbreaks through the Aotearoa Co-incidence Network (ACN): a population-based study. The Lancet Regional Health-Western Pacific, 20, 100351. Link\nStephens, J. M., Watson, P. W. S. J., Alansari, M., Lee, G., & Turnbull, S. M. (2021). Can Online Academic Integrity Instruction Affect University Students’ Perceptions of and Engagement in Academic Dishonesty? Results From a Natural Experiment in New Zealand. Frontiers in Psychology, 12, 366. Link\nTurnbull, S. M., & O’Neale, D. R. (2021). Entropy of Co-Enrolment Networks Reveal Disparities in High School STEM Participation. Frontiers in big Data, 3, 56. Link\nTurnbull, S. M., Meissel, K., Locke, K., & O’Neale, D. R. (2020, April). The Impact of Science Capital on Self-Concept in Science: A Study of University Students in New Zealand. In Frontiers in Education (Vol. 5, p. 27). Frontiers Link\nTurnbull, S. M., Locke, K., Vanholsbeeck, F., & O’Neale, D. R. (2019). Bourdieu, networks, and movements: Using the concepts of habitus, field and capital to understand a network analysis of gender differences in undergraduate physics. PloS one, 14(9), e0222357. Link"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Dr Steven Turnbull",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFourth Set: Nice, Easy Mapping\n\n\n\n\n\n\nR\n\n\nGeospatial\n\n\nNew Zealand\n\n\n\nIn this post, I try out a really easy way to make a nice minimalist map based on a set of coordinates.\n\n\n\n\n\nSep 5, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nThird Set: Point Density Mapping\n\n\n\n\n\n\nR\n\n\nGeospatial\n\n\nNew Zealand\n\n\n\nIn this post, I look at how you can use density functions to make a heat map of Tennis courts locations in Auckland, New Zealand.\n\n\n\n\n\nSep 5, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Set: Drive Isodistances\n\n\n\n\n\n\nR\n\n\nGeospatial\n\n\nNew Zealand\n\n\n\nIn this post, I look at how you can use road network data to get drive time radii around Tennis courts in Auckland, New Zealand.\n\n\n\n\n\nSep 2, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Set: Web Scraping and Geocoding\n\n\n\n\n\n\nR\n\n\nGeospatial\n\n\nNew Zealand\n\n\n\nIn this post, I look at how you can use a combination of webscraping and geocoding to get information on the location of Tennis courts in Auckland, New Zealand.\n\n\n\n\n\nAug 30, 2024\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tennis/SecondSet_Drive_Isodistances.html",
    "href": "posts/tennis/SecondSet_Drive_Isodistances.html",
    "title": "Second Set: Drive Isodistances",
    "section": "",
    "text": "In a previous post I outlined how we can take a set of addresses scraped from Tennis NZs website and geocode them to get latitude and longitude coordinates. This gave us the locations of all tennis clubs in Auckland, New Zealand. Now we have the data in a good format for mapping, we can carry out various forms of additional analysis.\nA couple of super powerful tools to have at our disposal are the isodistance and isochrone! Simply, an isodistance is a radius around a point on a map that indicates a distance away from that point. An isochrone is the same, but using travel time rather than distance. This will be impacted by various factors, such as proximity to motoways, rurality, geographic obstacles and so on.\nThere are various services that you can use to get this information, but I will show you how you can carry out this work yourself. Using R, we can save time by progamatically going through all of our data automatically, and we can save money (no API calls to services needed). We’re going to draw upon data on the New Zealand road network to carry out our work. Here’s a map I made earlier that visualises this network, which is based on the data provided by Beere, P., (2016).\n\n\n\nNew Zealand Road Network\n\n\nAs a first step, we’re going to load up the tennis club data.\n\n\nCode\nlibrary(here)\nlibrary(dplyr)\nlibrary(reactable)\ndf &lt;- read.csv(here(\"inputs\",\"tennis\",\"tennis_coords.csv\"))\n\n#We know the geocoding failed on some addresses, so we'll exclude those\ndf_clean &lt;- df |&gt;\n  filter(!is.na(latitude) | !is.na(longitude)) \n\ndf_clean |&gt; \n  head() |&gt;\n  reactable()"
  },
  {
    "objectID": "posts/tennis/SecondSet_Drive_Isodistances.html#the-shape-file-for-auckland-central.",
    "href": "posts/tennis/SecondSet_Drive_Isodistances.html#the-shape-file-for-auckland-central.",
    "title": "Second Set: Drive Isodistances",
    "section": "1: The shape file for Auckland Central.",
    "text": "1: The shape file for Auckland Central.\nThis is will be our map layer, showing where Auckland is.\n\n\nCode\nlibrary(sf)\nlibrary(stringr)\n\nnz_sf &lt;- read_sf(here(\"inputs\", \"shapefiles\", \"NZ_res01.shp\")) \n\n# Get the coordinates for Auckland in WGS84\nauckland_coords &lt;- matrix(\n  c(\n    174.5, -37.0,  # min x, min y (SW corner)\n    174.95, -37.0,  # max x, min y (SE corner)\n    174.95, -36.7,  # max x, max y (NE corner)\n    174.5, -36.7,  # min x, max y (NW corner)\n    174.5, -37.0   # closing point (SW corner)\n  ), \n  ncol = 2,\n  byrow = TRUE\n)\n\n# Create a POLYGON from the bounding box and set CRS to WGS84\nauckland_polygon &lt;- st_polygon(list(auckland_coords)) |&gt; \n  st_sfc(crs = 4326) \n\n# Load the shapefile and transform to NZTM2000 (EPSG: 2193)\nauckland_sf &lt;- nz_sf |&gt; \n  # Intersect the Auckland shapefile with the Auckland bounding box\n  st_intersection(auckland_polygon) |&gt;\n  st_transform(crs = 2193)"
  },
  {
    "objectID": "posts/tennis/SecondSet_Drive_Isodistances.html#well-need-the-road-network-data.",
    "href": "posts/tennis/SecondSet_Drive_Isodistances.html#well-need-the-road-network-data.",
    "title": "Second Set: Drive Isodistances",
    "section": "2: We’ll need the road network data.",
    "text": "2: We’ll need the road network data.\nWe’ll need to filter this data to only include roads within Auckland, and we’ll also need to make sure the Coordinate Reference System (CRS) is the same as our Auckland data. The CRS tells R which map projection to use, and if the layers in our data use different projections, the map won’t look correct. You can use the function st_crs(data) to get the CRS of an object, and the st_transform to set it as the same.\nAs you can see, the road network data contains a range of variables identifying what type of road it is, and where it is located. We’re going to filter out any roads that are not for cars.\n\n\nCode\n#Load our road data\nnz_roads &lt;- st_read(\n  here(\"inputs\",\"shapefiles\",\"NZ_roads.shp\"),\n  quiet = T\n  ) \n\n#set NZ roads CRS to be the same as the auckland sf.\n#nz_roads&lt;-st_transform(nz_roads, st_crs(auckland_polygon)) \n#roads_inside_auckland &lt;- st_intersection(nz_roads, auckland_polygon)\n\n#get the roads tidy\nauckland_roads &lt;- nz_roads |&gt;\n  filter(notforcar == 0) |&gt;\n  filter(region == \"auckland\") |&gt;\n  filter(!str_detect(label,\"auckland waiheke island ferry\"))  |&gt;\n  st_transform(crs = 2193) \n\nhead(auckland_roads) |&gt;\n  reactable()"
  },
  {
    "objectID": "posts/tennis/SecondSet_Drive_Isodistances.html#we-need-to-transform-our-tennis-club-location-data",
    "href": "posts/tennis/SecondSet_Drive_Isodistances.html#we-need-to-transform-our-tennis-club-location-data",
    "title": "Second Set: Drive Isodistances",
    "section": "3: We need to transform our tennis club location data",
    "text": "3: We need to transform our tennis club location data\nWe’ll need to make sure it’s in the same format and CRS.\n\n\nCode\n#clean point df and put to sf\npoint_sf &lt;- df_clean |&gt;\n  st_as_sf(\n    coords = c(\"longitude\", \"latitude\"),\n    crs = 4326\n  ) \n\npoint_sf_auckland &lt;- point_sf |&gt;\n  st_intersection(auckland_polygon) |&gt;\n  st_transform(crs = 2193)"
  },
  {
    "objectID": "posts/tennis/ThirdSet_Point_Density_Mapping.html",
    "href": "posts/tennis/ThirdSet_Point_Density_Mapping.html",
    "title": "Third Set: Point Density Mapping",
    "section": "",
    "text": "In a previous post I outlined how we can take a set of addresses scraped from Tennis NZs website and geocode them to get latitude and longitude coordinates. Following this, we can carry out various forms of additional analysis (such as mapping drive distances).\nIn this next post, I’ll show you how you can take any set of points with coordinates, and make a heat map across an area showing the density of points. This post is heavily inspired by work carried out by Andrew Heiss, check out his work here. the following post is very much an adaptation of his work!\nUsing density functions, we can visualise hot and cold spots across our desired area. In our case, we will be using the density point maps to understand which areas of Auckland have the greatest/least number of tennis clubs. As a first step, we’re going to load up the tennis club data. The code below will load our data in. For information on how this data was orinally source, see my post on webscraping and geocoding!\n\n\nCode\nlibrary(here)\nlibrary(dplyr)\nlibrary(reactable)\ndf &lt;- read.csv(here(\"inputs\",\"tennis\",\"tennis_coords.csv\"))\n\n#We know the geocoding failed on some addresses, so we'll exclude those\ndf_clean &lt;- df |&gt;\n  filter(!is.na(latitude) | !is.na(longitude)) \n\ndf_clean |&gt; \n  head() |&gt;\n  reactable(\n    bordered = TRUE,\n    highlight = TRUE\n    )"
  },
  {
    "objectID": "posts/tennis/ThirdSet_Point_Density_Mapping.html#load-the-shape-file-for-auckland-central.",
    "href": "posts/tennis/ThirdSet_Point_Density_Mapping.html#load-the-shape-file-for-auckland-central.",
    "title": "Third Set: Point Density Mapping",
    "section": "Load the shape file for Auckland Central.",
    "text": "Load the shape file for Auckland Central.\nThis is will be our map layer, showing where Auckland is.\n\n\nCode\nlibrary(sf)\nlibrary(stringr)\n\nnz_sf &lt;- read_sf(here(\"inputs\", \"shapefiles\", \"NZ_res01.shp\")) \n\n# Get the coordinates for Auckland in WGS84\nauckland_coords &lt;- matrix(\n  c(\n    174.5, -37.0,  # min x, min y (SW corner)\n    174.95, -37.0,  # max x, min y (SE corner)\n    174.95, -36.7,  # max x, max y (NE corner)\n    174.5, -36.7,  # min x, max y (NW corner)\n    174.5, -37.0   # closing point (SW corner)\n  ), \n  ncol = 2,\n  byrow = TRUE\n)\n\n# Create a POLYGON from the bounding box and set CRS to WGS84\nauckland_polygon &lt;- st_polygon(list(auckland_coords)) |&gt; \n  st_sfc(crs = 4326) \n\nauckland_sf &lt;- nz_sf |&gt; \n  # Intersect the Auckland shapefile with the Auckland bounding box\n  st_intersection(auckland_polygon) |&gt;\n  st_transform(crs = 2193)"
  },
  {
    "objectID": "posts/tennis/ThirdSet_Point_Density_Mapping.html#we-need-to-get-our-tennis-club-location-data",
    "href": "posts/tennis/ThirdSet_Point_Density_Mapping.html#we-need-to-get-our-tennis-club-location-data",
    "title": "Third Set: Point Density Mapping",
    "section": "2: We need to get our tennis club location data",
    "text": "2: We need to get our tennis club location data\nWe’ll need to make sure it’s in the same format and CRS.\n\n\nCode\n#clean point df and put to sf\npoint_sf &lt;- df_clean |&gt;\n  st_as_sf(\n    coords = c(\"longitude\", \"latitude\"),\n    crs = 4326\n  ) \n\npoint_sf_auckland &lt;- point_sf |&gt;\n  st_intersection(auckland_polygon) |&gt;\n  st_transform(crs = 2193) \n\n\nAs mentioned in Andrew Heiss’s blog post, we need to ensure that the coordinate reference system is one that uses metres. This is why I’ve set each of sf objects to have crs 2193 (New Zealand Transverse Mercator (NZTM))."
  }
]